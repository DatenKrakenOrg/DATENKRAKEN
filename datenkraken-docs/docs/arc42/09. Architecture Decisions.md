Design Decisions 
================
The following section shows our architectural decisions by sprints (initially introduced -> edits below), and their implication:

## Sprint 1

### ADR 1.1: Usage of NTP Server
<table>
  <thead>
    <tr>
      <th>Section</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Date</td>
      <td>17.07.2025</td>
    </tr>
    <tr>
      <td>Context</td>
      <td>The usage of the NTP server was first thought to be made on a edge device (f.e. ESP32). This was cancelled because of the following reason.</td>
    </tr>
    <tr>
      <td>Decision</td>
      <td>The decision was made since a NTP server call on the side of ESP32 would result in a loss of information (complete loss or inaccuracy of timestamp for collection time of a data point)</td>
    </tr>
    <tr>
      <td>Status</td>
      <td>Accepted</td>
    </tr>
    <tr>
      <td>Consequences</td>
      <td>It moves the implementation of a edge device to later phases of the project => This means that we may run out of time if we still want to use one.</td>
    </tr>
  </tbody>
</table>


### ADR 1.2: Subscription script in Persistence-Compose
<table>
  <thead>
    <tr>
      <th>Section</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Date</td>
      <td>17.07.2025</td>
    </tr>
    <tr>
      <td>Context</td>
      <td>A script is needed to forward the data from the mqtt broker towards the database. This step is done via a script within the persistence compose (the same as the database).</td>
    </tr>
    <tr>
      <td>Decision</td>
      <td>The decision a strong connectivity between database and the script since its held within the same network. Other than that the structure is more clear than holding it in two different composes.</td>
    </tr>
    <tr>
      <td>Status</td>
      <td>Accepted (04.08.2025)</td>
    </tr>
    <tr>
      <td>Consequences</td>
      <td>This means that if components of the compose must be replaced the script and/or the database must be stopped to. For now this should not be a problem since the mqtt broker can hold data until fetched (QOS). It's not 100% clear whether this setting is set, so this decision might change. => Accepted: Since a loss of a small portion of datapoints is not critical for our use case. The stability of the script is ensured by tests and a double auto reconnect logic of docker and paho-mqtt. Collisions with a duplicate broker client id were also tested, and do not result in a loss of data points, but only in a slight instability and flooding of the logs, which can be treated fairly well.</td>
    </tr>
  </tbody>
</table>

### ADR 1.3: TimescaleDB / Medaillon architecture of database
<table>
  <thead>
    <tr>
      <th>Section</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Date</td>
      <td>17.07.2025</td>
    </tr>
    <tr>
      <td>Context</td>
      <td>TimescaleDB is used for storing the time series sensor data. It's architecture is based on the medaillon architecture. Bronze layer is suppossed to be a table, but silver / gold layer is suppossed to be a materialized view.</td>
    </tr>
    <tr>
      <td>Decision</td>
      <td>The decision to use timescaledb is made since, all group members are familiar with sql, whilst still having sota timeseries performance. The medaillon architecture ensures NFR 1.2 (bronze layer) and NFR 1.3 (silver layer). A materialized view is used since it ensures that less overhead is needed (tables would propose replication), by that we can ensure NFR 1.2 by backing up the bronze layer table => Silver and Gold layer can be restored on compute time.</td>
    </tr>
    <tr>
      <td>Status</td>
      <td>Accepted</td>
    </tr>
    <tr>
      <td>Consequences</td>
      <td>This means that all of data cleaning must be done within the database. No external scripts can be used since of the usage of materialized views.</td>
    </tr>
  </tbody>
</table>

## Sprint 2

### ADR 2.1: Transmission Rate
<table>
  <thead>
    <tr>
      <th>Section</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Date</td>
      <td>22.07.2025</td>
    </tr>
    <tr>
      <td>Context</td>
      <td>We set the transmission rate of each message (by sensor) to a rate 30s / message</td>
    </tr>
    <tr>
      <td>Decision</td>
      <td>The decision was made by calculating the final data volume for a time period of 60 days (project run time)</td>
    </tr>
    <tr>
      <td>Status</td>
      <td>Accepted</td>
    </tr>
    <tr>
      <td>Consequences</td>
      <td>It implicates a aggregation of datapoints on periods of 30s intervalls. And a data volume of roughly 172k lines per table.</td>
    </tr>
  </tbody>
</table>

### ADR 2.2: Sampling Rate
<table>
  <thead>
    <tr>
      <th>Section</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Date</td>
      <td>22.07.2025</td>
    </tr>
    <tr>
      <td>Context</td>
      <td>We set the sampling rate of each sensor to: Sound Sensor: 1s -> Vector[30], Humidity: 30s -> Vector[1], Temperature: 30s -> Vector[1]
VOC: 5s -> Vector[6]</td>
    </tr>
    <tr>
      <td>Decision</td>
      <td>The decision was made by calculating the estimating the probability of outliers for each sensor (in order to have enough data to aggregate data to reduce outliers)</td>
    </tr>
    <tr>
      <td>Status</td>
      <td>Accepted</td>
    </tr>
    <tr>
      <td>Consequences</td>
      <td>A loss of information in the time between the samples.</td>
    </tr>
  </tbody>
</table>

### ADR 2.3: Multi Table Timescale Setup
<table>
  <thead>
    <tr>
      <th>Section</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Date</td>
      <td>24.07.2025</td>
    </tr>
    <tr>
      <td>Context</td>
      <td>We use a hypertable in our database for each sensor.</td>
    </tr>
    <tr>
      <td>Decision</td>
      <td>The decision was made since otherwise we introduce many null values since our sampling rate of the sensors are different. This is a problem since timescale interprets null as a actual value.</td>
    </tr>
    <tr>
      <td>Status</td>
      <td>Accepted</td>
    </tr>
    <tr>
      <td>Consequences</td>
      <td>Slighly more difficult joining strategies may be needed.</td>
    </tr>
  </tbody>
</table>

### ADR 2.4: Composite index
<table>
  <thead>
    <tr>
      <th>Section</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Date</td>
      <td>24.07.2025</td>
    </tr>
    <tr>
      <td>Context</td>
      <td>We create a composite index on arduino_id and time.</td>
    </tr>
    <tr>
      <td>Decision</td>
      <td>Since our use case will need both columns for filtering often we introduce composite index on all layers => Allows fast tracing of values from gold to bronze layer and vice versa.</td>
    </tr>
    <tr>
      <td>Status</td>
      <td>Accepted</td>
    </tr>
    <tr>
      <td>Consequences</td>
      <td>Slightly less insert performance (negligible with our transmission rate), higher memory usage</td>
    </tr>
  </tbody>
</table>

## Sprint 3
### ADR 3.1: Composite primary key
<table>
  <thead>
    <tr>
      <th>Section</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Date</td>
      <td>31.07.2025</td>
    </tr>
    <tr>
      <td>Context</td>
      <td>We create a primary key (+composite index) on id and time.</td>
    </tr>
    <tr>
      <td>Decision</td>
      <td>Since sqlalchemy requires a primary key in order to work we introduced a primary key in our table. This is no anti pattern when using timescale if a composite primary key (which includes the time) is used.</td>
    </tr>
    <tr>
      <td>Status</td>
      <td>Accepted</td>
    </tr>
    <tr>
      <td>Consequences</td>
      <td>Slightly more memory usage (esp. since additional index). </td>
    </tr>
  </tbody>
</table>

### ADR 3.2: Cron Job Backup
<table>
  <thead>
    <tr>
      <th>Section</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Date</td>
      <td>04.08.2025</td>
    </tr>
    <tr>
      <td>Context</td>
      <td>We use a cron job to backup our database.</td>
    </tr>
    <tr>
      <td>Decision</td>
      <td>Since the backup has to be created periodically and be saved by hand regularly a cron job was chosen as our backup strategy. It creates a pgdump which is archived and persisted on a private cloud storage. The size was estimated on a full backup with around 600mb.</td>
    </tr>
    <tr>
      <td>Status</td>
      <td>Accepted</td>
    </tr>
    <tr>
      <td>Consequences</td>
      <td>If a backup is missing, the whole data is lost since it's not a incremental backup. But the chance is pretty low since it lays on a private cloud storage.</td>
    </tr>
  </tbody>
</table>

### ADR 3.3: Cron Job Uptime
<table>
  <thead>
    <tr>
      <th>Section</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Date</td>
      <td>04.08.2025</td>
    </tr>
    <tr>
      <td>Context</td>
      <td>We use a cron job to track our uptime.</td>
    </tr>
    <tr>
      <td>Decision</td>
      <td>Since we just have to track our uptime for up to one week we decided to use a cron job to fetch the database uptime and store it within a csv file.</td>
    </tr>
    <tr>
      <td>Status</td>
      <td>Accepted</td>
    </tr>
    <tr>
      <td>Consequences</td>
      <td>At first glance we do not have any dashboard (like in more complicated containerized methods). But we do have control over the full configuration and what we do with it later on.</td>
    </tr>
  </tbody>
</table>

### ADR 3.4: Architectural Decisions
<table>
  <thead>
    <tr>
      <th>Section</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Date</td>
      <td>04.08.2025</td>
    </tr>
    <tr>
      <td>Context</td>
      <td>We introduce a deleted_at column in bronze layer.</td>
    </tr>
    <tr>
      <td>Decision</td>
      <td>We use soft deletes in our bronze layer in order to not lose any data.</td>
    </tr>
    <tr>
      <td>Status</td>
      <td>Accepted</td>
    </tr>
    <tr>
      <td>Consequences</td>
      <td>Higher memory usage => But our memory usage is overall pretty low therefore we do not want to lose data if not necessary.</td>
    </tr>
  </tbody>
</table>

## Sprint 4:
### ADR 4.1: Sensor data aggreggation intervalls (silver)

<table>
  <thead>
    <tr>
      <th>Section</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Date</td>
      <td>13.08.2025</td>
    </tr>
    <tr>
      <td>Context</td>
      <td>In order to reduce memory layerwise we propose a aggregation via average in certain time intervalls in a way we can quantify information loss. => Variational Coefficient (see Chap: <a href="/DATENKRAKEN/indepth/database/">Database</a>)</td>
    </tr>
    <tr>
      <td>Decision</td>
      <td>Temperature & Humidity: 15 Minutes, Noise: 30 Sec, Voc: 5 Minutes</td>
    </tr>
    <tr>
      <td>Status</td>
      <td>Accepted</td>
    </tr>
    <tr>
      <td>Consequences</td>
      <td>We may lose information of local trends for our machine learning use case (which could potentially need further information for f.e. moving avgs or other rolling features) but since the time seems not to allow our ml use case this is doesn't propose a risk. And even if we need this kind of information of local trends we can still choose a smaller intervall since all layers besides bronze is constructed as a view.</td>
    </tr>
  </tbody>
</table>